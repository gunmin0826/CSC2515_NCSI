{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Fourier Features (RFF) for Kernel Ridge Regression\n",
    "\n",
    "## 1. Kernel Ridge Regression (KRR)\n",
    "Kernel Ridge Regression solves the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha} \\|Y - K \\alpha\\|^2 + \\lambda \\|\\alpha\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Y$ is the target vector of shape $(n, 1)$,\n",
    "- $K$ is the kernel matrix of shape $(n, n)$, defined by $K_{ij} = k(x_i, x_j)$,\n",
    "- $\\lambda$ is the regularization parameter,\n",
    "- $\\alpha$ is the vector of model coefficients.\n",
    "\n",
    "The kernel function $k(x, z)$ for the Gaussian (RBF) kernel is defined as:\n",
    "\n",
    "$$\n",
    "k(x, z) = \\exp\\left(-\\frac{\\|x - z\\|^2}{2 \\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The solution to this problem is:\n",
    "\n",
    "$$\n",
    "\\alpha = (K + \\lambda I)^{-1} Y\n",
    "$$\n",
    "\n",
    "Given $\\alpha$, predictions for a new input $x$ are:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{i=1}^n \\alpha_i k(x, x_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Motivation for Random Fourier Features\n",
    "Computing the kernel matrix $K$ is expensive for large datasets, as it scales quadratically with the number of samples $n$. To approximate the kernel, we use Random Fourier Features, based on the following result:\n",
    "\n",
    "### Bochner's Theorem\n",
    "For a shift-invariant kernel $k(x, z) = k(x - z)$, there exists a probability distribution $p(\\omega)$ such that:\n",
    "\n",
    "$$\n",
    "k(x, z) = \\int_{\\mathbb{R}^d} p(\\omega) e^{j \\omega^\\top (x - z)} d\\omega\n",
    "$$\n",
    "\n",
    "For the Gaussian kernel:\n",
    "\n",
    "$$\n",
    "k(x, z) = \\exp\\left(-\\frac{\\|x - z\\|^2}{2 \\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The corresponding distribution $p(\\omega)$ is Gaussian:\n",
    "\n",
    "$$\n",
    "\\omega \\sim \\mathcal{N}(0, \\frac{1}{\\sigma^2} I)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Random Fourier Feature Approximation\n",
    "We approximate the kernel by sampling $\\omega$ from $p(\\omega)$. Using Monte Carlo sampling, we can approximate $k(x, z)$ as:\n",
    "\n",
    "$$\n",
    "k(x, z) \\approx \\phi(x)^\\top \\phi(z)\n",
    "$$\n",
    "\n",
    "where $\\phi(x)$ is the Random Fourier Feature mapping:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\sqrt{\\frac{2}{D}} \\cos(Wx + b)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $W$ is a matrix of random samples $\\omega \\sim \\mathcal{N}(0, \\frac{1}{\\sigma^2} I)$,\n",
    "- $b$ is a vector of random offsets $b \\sim \\text{Uniform}(0, 2\\pi)$,\n",
    "- $D$ is the number of random features.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Kernel Ridge Regression with RFF\n",
    "Using the RFF mapping $\\phi(x)$, we approximate the kernel matrix $K$ as:\n",
    "\n",
    "$$\n",
    "K \\approx \\Phi \\Phi^\\top\n",
    "$$\n",
    "\n",
    "where $\\Phi$ is the feature matrix after applying $\\phi(x)$ to all data points $X$.\n",
    "\n",
    "### Modified Objective Function\n",
    "We now rewrite the KRR optimization problem using $\\Phi$:\n",
    "\n",
    "$$\n",
    "\\min_w \\|Y - \\Phi w\\|^2 + \\lambda \\|w\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w$ is the weight vector in the transformed RFF space.\n",
    "\n",
    "### Solution\n",
    "The solution for $w$ is given by:\n",
    "\n",
    "$$\n",
    "w = (\\Phi^\\top \\Phi + \\lambda I)^{-1} \\Phi^\\top Y\n",
    "$$\n",
    "\n",
    "### Prediction\n",
    "For a new input $x'$, the prediction is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\phi(x')^\\top w\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Advantages of RFF\n",
    "1. **Efficiency**:\n",
    "   - Avoids computing and storing the $n \\times n$ kernel matrix.\n",
    "   - Instead, transforms the data into an $n \\times D$ feature matrix, where $D$ is typically much smaller than $n$.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - Linear in $n$ for training and predictions.\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - Can use any linear solver (e.g., Ridge Regression) on the transformed features.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/home/scott/CSC2515_NCSI/Kernel/qm9_coulomb_matrix.npz', allow_pickle=True)\n",
    "features = data['features']\n",
    "labels = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFourierFeatures:\n",
    "    def __init__(self, input_dim, num_features, sigma):\n",
    "        \"\"\"\n",
    "        Initialize the RFF transformer.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_dim: Dimensionality of the input features.\n",
    "        - num_features: Number of random Fourier features (D).\n",
    "        - sigma: Kernel bandwidth (scale parameter for Gaussian kernel).\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.sigma = sigma\n",
    "        self.W = np.random.normal(loc=0, scale=1/sigma, size=(input_dim, num_features))\n",
    "        self.b = np.random.uniform(0, 2 * np.pi, size=num_features)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input features to the RFF feature space.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input features, shape (n_samples, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "        - Transformed features, shape (n_samples, num_features).\n",
    "        \"\"\"\n",
    "        projection = X @ self.W + self.b\n",
    "        return np.sqrt(2 / self.num_features) * np.cos(projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KRR_with_RFF(X_train_rff, y_train, lam, num_features=500):\n",
    "    \"\"\"\n",
    "    Kernel Ridge Regression using Random Fourier Features (RFF).\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training data, shape (n_train, input_dim).\n",
    "    - y_train: Training labels, shape (n_train,).\n",
    "    - X_test: Test data, shape (n_test, input_dim).\n",
    "    - lam: Regularization parameter (lambda).\n",
    "    - sigma: Bandwidth for the Gaussian kernel.\n",
    "    - num_features: Number of random Fourier features to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - Predictions on the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ridge regression in transformed space\n",
    "    K = X_train_rff.T @ X_train_rff + lam * np.eye(num_features)\n",
    "    try:\n",
    "        alphas = np.linalg.solve(K, X_train_rff.T @ y_train)\n",
    "    except:\n",
    "        return 'Solve failed, check pairwise distances'\n",
    "    else:\n",
    "        return alphas\n",
    "\n",
    "def KRR(X_train, y_train, lam, sigma):\n",
    "    \"\"\"\n",
    "    Kernel Ridge Regression using Random Fourier Features (RFF).\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training data, shape (n_train, input_dim).\n",
    "    - y_train: Training labels, shape (n_train,).\n",
    "    - X_test: Test data, shape (n_test, input_dim).\n",
    "    - lam: Regularization parameter (lambda).\n",
    "    - sigma: Bandwidth for the Gaussian kernel.\n",
    "    - num_features: Number of random Fourier features to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - Predictions on the test data.\n",
    "    \"\"\"\n",
    "    dist = pairwise_distances(X_train)\n",
    "    exponent = dist/sigma\n",
    "\n",
    "    K = np.exp(-(exponent**2)/2)\n",
    "    K += np.eye(K.shape[0])*lam\n",
    "    \n",
    "    try:\n",
    "        alphas = np.linalg.solve(K, y_train)\n",
    "    except:\n",
    "        return 'Solve failed, check pairwise distances'\n",
    "    else:\n",
    "        return alphas\n",
    "\n",
    "def GridSearchCV(X, Y, params, num_features, cv=4, train_set_ratio=1):\n",
    "    print(params)\n",
    "    kf = KFold(n_splits=cv)\n",
    "    X_train, X_test = [], []\n",
    "    Y_train, Y_test = [], []\n",
    "    for train, test in kf.split(X):\n",
    "        train_size = int(train_set_ratio*len(train))\n",
    "        sampled_train = np.random.choice(train, train_size, replace=False)\n",
    "        X_train.append(X[sampled_train])\n",
    "        Y_train.append(Y[sampled_train])\n",
    "        X_test.append(X[test])\n",
    "        Y_test.append(Y[test])\n",
    "\n",
    "    best_score = np.inf  # Maximizing accuracy\n",
    "    mat = np.zeros((len(params['lambda']), len(params['length'])))\n",
    "    for i, lam in enumerate(params['lambda']):\n",
    "        for j, sigma in enumerate(params['length']):\n",
    "            maes = []\n",
    "            for k in range(cv):\n",
    "                #rff = RandomFourierFeatures(input_dim=X_train[k].shape[1], num_features=num_features, sigma=sigma)\n",
    "        \n",
    "                alphas = KRR(X_train[k], Y_train[k], lam, sigma)\n",
    "                #alphas = KRR_with_RFF(rff.transform(X_train[k]), Y_train[k], lam, num_features)\n",
    "\n",
    "                if type(alphas) == str:\n",
    "                    mae = np.inf\n",
    "                else:\n",
    "                    # normal KRR\n",
    "                    dist = scipy.spatial.distance_matrix(X_train[k], X_test[k])\n",
    "                    exponent = dist/sigma\n",
    "                \n",
    "                    K_test = np.exp(-(exponent**2)/2)\n",
    "                    preds = K_test.T @ alphas\n",
    "                    mae = np.mean(np.abs(preds - Y_test[k]))\n",
    "\n",
    "                    # RFF\n",
    "                    #X_test_rff = rff.transform(X_test[k])\n",
    "#\n",
    "                    #preds = X_test_rff @ alphas\n",
    "                    #mae = np.mean(np.abs(preds - Y_test[k]))\n",
    "\n",
    "                maes.append(mae)\n",
    "            mat[i][j] = np.mean(maes)\n",
    "            if np.mean(maes) < best_score:\n",
    "                best_score = np.mean(maes)\n",
    "                best_lambda = lam\n",
    "                best_sigma = sigma\n",
    "    \n",
    "    plt.imshow(mat, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.colorbar(label=\"MAE\")\n",
    "    plt.xlabel(\"$\\sigma$\", fontsize=14)\n",
    "    plt.ylabel(\"$\\lambda$\", fontsize=14)\n",
    "    plt.xticks(np.arange(len(params['length'])), [f'{sigma:.2f}' for sigma in params['length']], rotation=45)\n",
    "    plt.yticks(np.arange(len(params['lambda'])), [f'{lam:.2e}' for lam in params['lambda']])\n",
    "    plt.title(f\"Grid Search Accuracy Heatmap\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    return {'accuracy': best_score, 'lambda': best_lambda, 'length': best_sigma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fractions = [0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "test_size = int((1-max(training_fractions))*len(features))\n",
    "\n",
    "N = 2\n",
    "all_maes = []\n",
    "\n",
    "time_to_invert = np.zeros((N,len(training_fractions)))\n",
    "time_to_predict = np.zeros((N,len(training_fractions)))\n",
    "\n",
    "for n in range(N):\n",
    "    indices = list(range(len(features)))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    X = features[indices]\n",
    "    Y = labels[indices]\n",
    "\n",
    "    X_test = X[-test_size:]\n",
    "    Y_test = Y[-test_size:]\n",
    "\n",
    "    X_trains, Y_trains, = [], []\n",
    "\n",
    "    for frac in training_fractions:\n",
    "        training_size = int(frac*len(X))\n",
    "        X_trains.append(X[:training_size])\n",
    "        Y_trains.append(Y[:training_size])\n",
    "\n",
    "    maes = []\n",
    "    if n == 1:\n",
    "        for i in range(len(training_fractions)):\n",
    "            print(f'Training size {training_fractions[i]*len(X)}')\n",
    "            X_train = X_trains[i]\n",
    "            Y_train = Y_trains[i]\n",
    "\n",
    "            param_grid = {'length': np.logspace(1.5, 3.5, num=5),\n",
    "                        'lambda': np.logspace(-12, -4, num=5)}\n",
    "            #best_params = GridSearchCV(X_train, Y_train, param_grid, 4, 0.5)\n",
    "            best_params = {'lambda': 1e-08, 'length': 3162.2776601683795}\n",
    "            print(best_params)\n",
    "\n",
    "            #rff = RandomFourierFeatures(input_dim=X_train.shape[1], num_features=num_features, sigma=sigma)\n",
    "\n",
    "            begin = time.time()\n",
    "            alphas = KRR(X_train, Y_train, best_params['lambda'], best_params['length'])\n",
    "            # alphas = KRR_with_RFF(rff.transform(X_train), Y_train, best_params['lambda'], num_features)\n",
    "            end = time.time()\n",
    "            kernel_inversion = end - begin\n",
    "            time_to_invert[n,i] = end - begin\n",
    "\n",
    "            # normal KRR\n",
    "            begin2 = time.time()\n",
    "            dist = scipy.spatial.distance_matrix(X_test,X_train)\n",
    "            exponent = dist/best_params['length']\n",
    "            K_test = np.exp(-(exponent**2)/2)\n",
    "            preds = np.dot(K_test,alphas)\n",
    "            end2 = time.time()\n",
    "            pred_time = end2 - begin2\n",
    "            time_to_predict[n,i] = end2 - begin2\n",
    "\n",
    "            maes.append(np.mean(np.abs(preds - Y_test)))\n",
    "\n",
    "            # RFF\n",
    "            #begin2 = time.time()\n",
    "            #rff = RandomFourierFeatures(input_dim=X_train.shape[1], num_features=num_features, sigma=best_params['length'])\n",
    "            #X_test_rff = rff.transform(X_test)\n",
    "            #preds = X_test_rff @ alphas\n",
    "            #end2 = time.time()\n",
    "            #pred_time = end2 - begin2\n",
    "\n",
    "            #mae = np.mean(np.abs(preds - Y_test))\n",
    "            print(f'Kernel inversion took {kernel_inversion}, predictions {pred_time}')                                                                                       \n",
    "\n",
    "    all_maes.append(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(features, labels, N, method, num_features=None):\n",
    "    training_fractions = [0.05, 0.1, 0.2, 0.4]\n",
    "    test_size = int((1-max(training_fractions))*len(features))\n",
    "\n",
    "    # N = 2\n",
    "    all_maes = []\n",
    "\n",
    "    time_to_invert = np.zeros((N,len(training_fractions)))\n",
    "    time_to_predict = np.zeros((N,len(training_fractions)))\n",
    "    maes = np.full((N,len(training_fractions)), np.inf, dtype=np.float32)\n",
    "\n",
    "    for n in range(N):\n",
    "        indices = list(range(len(features)))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        X = features[indices]\n",
    "        Y = labels[indices]\n",
    "\n",
    "        X_test = X[-test_size:]\n",
    "        Y_test = Y[-test_size:]\n",
    "\n",
    "        X_trains, Y_trains, = [], []\n",
    "\n",
    "        for frac in training_fractions:\n",
    "            training_size = int(frac*len(X))\n",
    "            X_trains.append(X[:training_size])\n",
    "            Y_trains.append(Y[:training_size])\n",
    "\n",
    "       \n",
    "        \n",
    "        for i in range(len(training_fractions)):\n",
    "\n",
    "            print(f'Training size {training_fractions[i]*len(X)}')\n",
    "            X_train = X_trains[i]\n",
    "            Y_train = Y_trains[i]\n",
    "\n",
    "            param_grid = {'length': np.logspace(1.5, 3.5, num=5),\n",
    "                        'lambda': np.logspace(-12, -4, num=5)}\n",
    "            best_params = GridSearchCV(X_train, Y_train, param_grid,num_features, 4, 0.5)\n",
    "            print(best_params)\n",
    "\n",
    "            \n",
    "            if method == 'KRR':\n",
    "                begin = time.time()\n",
    "                alphas = KRR(X_train, Y_train, best_params['lambda'], best_params['length'])\n",
    "                end = time.time()\n",
    "                time_to_invert[n,i] = end - begin\n",
    "\n",
    "                # normal KRR\n",
    "                begin2 = time.time()\n",
    "                dist = scipy.spatial.distance_matrix(X_test,X_train)\n",
    "                exponent = dist/best_params['length']\n",
    "                K_test = np.exp(-(exponent**2)/2)\n",
    "                preds = np.dot(K_test,alphas)\n",
    "                end2 = time.time()\n",
    "                pred_time = end2 - begin2\n",
    "                time_to_predict[n,i] = end2 - begin2\n",
    "\n",
    "                print(f'Kernel inversion took {kernel_inversion}, predictions {pred_time}') \n",
    "\n",
    "                maes[n,i] = np.mean(np.abs(preds - Y_test))\n",
    "\n",
    "\n",
    "            elif method == 'RFF':\n",
    "                if num_features is None:\n",
    "                    print(\"you forgot num_features\")\n",
    "                    return\n",
    "                rff = RandomFourierFeatures(input_dim=X_train.shape[1], num_features=num_features, sigma=best_params['length'])\n",
    "\n",
    "                begin = time.time()\n",
    "                alphas = KRR_with_RFF(rff.transform(X_train), Y_train, best_params['lambda'], num_features)\n",
    "                end = time.time()\n",
    "                kernel_inversion = end-begin\n",
    "                time_to_invert[n,i] = end - begin\n",
    "\n",
    "                # RFF\n",
    "                begin2 = time.time()\n",
    "                X_test_rff = rff.transform(X_test)\n",
    "                preds = X_test_rff @ alphas\n",
    "                end2 = time.time()\n",
    "                pred_time = end2-begin2\n",
    "                time_to_predict[n,i] = end2 - begin2\n",
    "\n",
    "                print(f'Kernel inversion took {kernel_inversion}, predictions {pred_time}') \n",
    "                maes[n,i] = np.mean(np.abs(preds - Y_test)) \n",
    "\n",
    "            else:\n",
    "                print(\"please select correct method\")\n",
    "                return\n",
    "            \n",
    "            # kernel_inversion = end - begin\n",
    "            \n",
    "    if method == 'KRR':\n",
    "        np.savez('time_diff/KRR.npz', maes=maes, time_to_invert=time_to_invert, time_to_predict=time_to_predict)\n",
    "\n",
    "            \n",
    "    else:\n",
    "        np.savez('time_diff/RFF_n'+str(num_features) +'.npz', maes=maes, time_to_invert=time_to_invert, time_to_predict=time_to_predict)\n",
    "\n",
    "            \n",
    "    return maes, time_to_invert, time_to_predict\n",
    "            #mae = np.mean(np.abs(preds - Y_test))\n",
    "            # print(f'Kernel inversion took {kernel_inversion}, predictions {pred_time}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes, time_to_invert, time_to_predict = measure_time(features, labels, N=20, method='RFF', num_features=1000) # Scott"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
