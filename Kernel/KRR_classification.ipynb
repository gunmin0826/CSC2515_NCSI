{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.io.arff import loadarff \n",
    "import time\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Fourier Features (RFF) for Kernel Ridge Regression\n",
    "\n",
    "## 1. Kernel Ridge Regression (KRR)\n",
    "Kernel Ridge Regression solves the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha} \\|Y - K \\alpha\\|^2 + \\lambda \\|\\alpha\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Y$ is the target vector of shape $(n, 1)$,\n",
    "- $K$ is the kernel matrix of shape $(n, n)$, defined by $K_{ij} = k(x_i, x_j)$,\n",
    "- $\\lambda$ is the regularization parameter,\n",
    "- $\\alpha$ is the vector of model coefficients.\n",
    "\n",
    "The kernel function $k(x, z)$ for the Gaussian (RBF) kernel is defined as:\n",
    "\n",
    "$$\n",
    "k(x, z) = \\exp\\left(-\\frac{\\|x - z\\|^2}{2 \\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The solution to this problem is:\n",
    "\n",
    "$$\n",
    "\\alpha = (K + \\lambda I)^{-1} Y\n",
    "$$\n",
    "\n",
    "Given $\\alpha$, predictions for a new input $x$ are:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{i=1}^n \\alpha_i k(x, x_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Motivation for Random Fourier Features\n",
    "Computing the kernel matrix $K$ is expensive for large datasets, as it scales quadratically with the number of samples $n$. To approximate the kernel, we use Random Fourier Features, based on the following result:\n",
    "\n",
    "### Bochner's Theorem\n",
    "For a shift-invariant kernel $k(x, z) = k(x - z)$, there exists a probability distribution $p(\\omega)$ such that:\n",
    "\n",
    "$$\n",
    "k(x, z) = \\int_{\\mathbb{R}^d} p(\\omega) e^{j \\omega^\\top (x - z)} d\\omega\n",
    "$$\n",
    "\n",
    "For the Gaussian kernel:\n",
    "\n",
    "$$\n",
    "k(x, z) = \\exp\\left(-\\frac{\\|x - z\\|^2}{2 \\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The corresponding distribution $p(\\omega)$ is Gaussian:\n",
    "\n",
    "$$\n",
    "\\omega \\sim \\mathcal{N}(0, \\frac{1}{\\sigma^2} I)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Random Fourier Feature Approximation\n",
    "We approximate the kernel by sampling $\\omega$ from $p(\\omega)$. Using Monte Carlo sampling, we can approximate $k(x, z)$ as:\n",
    "\n",
    "$$\n",
    "k(x, z) \\approx \\phi(x)^\\top \\phi(z)\n",
    "$$\n",
    "\n",
    "where $\\phi(x)$ is the Random Fourier Feature mapping:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\sqrt{\\frac{2}{D}} \\cos(Wx + b)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $W$ is a matrix of random samples $\\omega \\sim \\mathcal{N}(0, \\frac{1}{\\sigma^2} I)$,\n",
    "- $b$ is a vector of random offsets $b \\sim \\text{Uniform}(0, 2\\pi)$,\n",
    "- $D$ is the number of random features.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Kernel Ridge Regression with RFF\n",
    "Using the RFF mapping $\\phi(x)$, we approximate the kernel matrix $K$ as:\n",
    "\n",
    "$$\n",
    "K \\approx \\Phi \\Phi^\\top\n",
    "$$\n",
    "\n",
    "where $\\Phi$ is the feature matrix after applying $\\phi(x)$ to all data points $X$.\n",
    "\n",
    "### Modified Objective Function\n",
    "We now rewrite the KRR optimization problem using $\\Phi$:\n",
    "\n",
    "$$\n",
    "\\min_w \\|Y - \\Phi w\\|^2 + \\lambda \\|w\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w$ is the weight vector in the transformed RFF space.\n",
    "\n",
    "### Solution\n",
    "The solution for $w$ is given by:\n",
    "\n",
    "$$\n",
    "w = (\\Phi^\\top \\Phi + \\lambda I)^{-1} \\Phi^\\top Y\n",
    "$$\n",
    "\n",
    "### Prediction\n",
    "For a new input $x'$, the prediction is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\phi(x')^\\top w\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Advantages of RFF\n",
    "1. **Efficiency**:\n",
    "   - Avoids computing and storing the $n \\times n$ kernel matrix.\n",
    "   - Instead, transforms the data into an $n \\times D$ feature matrix, where $D$ is typically much smaller than $n$.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - Linear in $n$ for training and predictions.\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - Can use any linear solver (e.g., Ridge Regression) on the transformed features.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arff_file = loadarff('/home/scott/CSC2515_NCSI/Kernel/dataset/MagicTelescope.arff')\n",
    "data = pd.DataFrame(arff_file[0])\n",
    "data['labels'] = (data['class:'] == b'g').astype(int)\n",
    "labels = data['labels'].to_numpy()\n",
    "features = data.iloc[:,1:-2].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFourierFeatures:\n",
    "    def __init__(self, input_dim, num_features, sigma):\n",
    "        \"\"\"\n",
    "        Initialize the RFF transformer.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_dim: Dimensionality of the input features.\n",
    "        - num_features: Number of random Fourier features (D).\n",
    "        - sigma: Kernel bandwidth (scale parameter for Gaussian kernel).\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.sigma = sigma\n",
    "        self.W = np.random.normal(loc=0, scale=1/sigma, size=(input_dim, num_features))\n",
    "        self.b = np.random.uniform(0, 2 * np.pi, size=num_features)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input features to the RFF feature space.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input features, shape (n_samples, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "        - Transformed features, shape (n_samples, num_features).\n",
    "        \"\"\"\n",
    "        projection = X @ self.W + self.b\n",
    "        return np.sqrt(2 / self.num_features) * np.cos(projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KRR_with_RFF(X_train_rff, y_train, lam, num_features=500):\n",
    "    \"\"\"\n",
    "    Kernel Ridge Regression using Random Fourier Features (RFF).\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training data, shape (n_train, input_dim).\n",
    "    - y_train: Training labels, shape (n_train,).\n",
    "    - X_test: Test data, shape (n_test, input_dim).\n",
    "    - lam: Regularization parameter (lambda).\n",
    "    - sigma: Bandwidth for the Gaussian kernel.\n",
    "    - num_features: Number of random Fourier features to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - Predictions on the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ridge regression in transformed space\n",
    "    K = X_train_rff.T @ X_train_rff + lam * np.eye(num_features)\n",
    "    try:\n",
    "        alphas = np.linalg.solve(K, X_train_rff.T @ y_train)\n",
    "    except:\n",
    "        return 'Solve failed, check pairwise distances'\n",
    "    else:\n",
    "        return alphas\n",
    "\n",
    "def KRR(X_train, y_train, lam, sigma):\n",
    "    \"\"\"\n",
    "    Kernel Ridge Regression using Random Fourier Features (RFF).\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training data, shape (n_train, input_dim).\n",
    "    - y_train: Training labels, shape (n_train,).\n",
    "    - X_test: Test data, shape (n_test, input_dim).\n",
    "    - lam: Regularization parameter (lambda).\n",
    "    - sigma: Bandwidth for the Gaussian kernel.\n",
    "    - num_features: Number of random Fourier features to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - Predictions on the test data.\n",
    "    \"\"\"\n",
    "    dist = pairwise_distances(X_train)\n",
    "    exponent = dist/sigma\n",
    "\n",
    "    K = np.exp(-(exponent**2)/2)\n",
    "    K += np.eye(K.shape[0])*lam\n",
    "    \n",
    "    try:\n",
    "        alphas = np.linalg.solve(K, y_train)\n",
    "    except:\n",
    "        return 'Solve failed, check pairwise distances'\n",
    "    else:\n",
    "        return alphas\n",
    "\n",
    "def GridSearchCV(X, Y, params, num_features, cv=4, train_set_ratio=1):\n",
    "    kf = KFold(n_splits=cv)\n",
    "    X_train, X_test = [], []\n",
    "    Y_train, Y_test = [], []\n",
    "    for train, test in kf.split(X):\n",
    "        train_size = int(train_set_ratio*len(train))\n",
    "        sampled_train = np.random.choice(train, train_size, replace=False)\n",
    "        X_train.append(X[sampled_train])\n",
    "        Y_train.append(Y[sampled_train])\n",
    "        X_test.append(X[test])\n",
    "        Y_test.append(Y[test])\n",
    "\n",
    "    best_score = 0  # Maximizing accuracy\n",
    "    mat = np.zeros((len(params['lambda']), len(params['length'])))\n",
    "    for i, lam in enumerate(params['lambda']):\n",
    "        for j, sigma in enumerate(params['length']):\n",
    "            accuracies = []\n",
    "            for k in range(cv):\n",
    "                rff = RandomFourierFeatures(input_dim=X_train[k].shape[1], num_features=num_features, sigma=sigma)\n",
    "\n",
    "                #alphas = KRR(X_train[k], Y_train[k], lam, sigma)\n",
    "                alphas = KRR_with_RFF(rff.transform(X_train[k]), Y_train[k], lam, num_features)\n",
    "\n",
    "                if type(alphas) == str:\n",
    "                    accuracy = 0\n",
    "                else:\n",
    "                    # normal KRR\n",
    "                    #dist = scipy.spatial.distance_matrix(X_train[k], X_test[k])\n",
    "                    #exponent = dist/sigma\n",
    "                #\n",
    "                    #K_test = np.exp(-(exponent**2)/2)\n",
    "                    #preds = (K_test.T @ alphas) > 0.5\n",
    "                    #accuracy = accuracy_score(preds,  Y_test[k])\n",
    "\n",
    "                    # RFF\n",
    "                    X_test_rff = rff.transform(X_test[k])\n",
    "                    preds = (X_test_rff @ alphas) > 0.5\n",
    "                    accuracy = accuracy_score(preds, Y_test[k])\n",
    "\n",
    "                accuracies.append(accuracy)\n",
    "            mat[i][j] = np.mean(accuracies)\n",
    "            if np.mean(accuracies) > best_score:\n",
    "                best_score = np.mean(accuracies)\n",
    "                best_lambda = lam\n",
    "                best_sigma = sigma\n",
    "    \n",
    "    #plt.imshow(mat, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    #plt.colorbar(label=\"Accuracy\")\n",
    "    #plt.xlabel(\"$\\sigma$\", fontsize=14)\n",
    "    #plt.ylabel(\"$\\lambda$\", fontsize=14)\n",
    "    #plt.xticks(np.arange(len(params['length'])), [f'{sigma:.2f}' for sigma in params['length']], rotation=45)\n",
    "    #plt.yticks(np.arange(len(params['lambda'])), [f'{lam:.2e}' for lam in params['lambda']])\n",
    "    #plt.title(f\"Grid Search Accuracy Heatmap\", fontsize=16)\n",
    "    #plt.show()\n",
    "\n",
    "    return {'accuracy': best_score, 'lambda': best_lambda, 'length': best_sigma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fractions = [0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "test_size = int((1-max(training_fractions))*len(features))\n",
    "\n",
    "num_features = 1\n",
    "\n",
    "N = 10\n",
    "all_accuracies = []\n",
    "all_kernel_inversions = []\n",
    "all_pred_times = []\n",
    "\n",
    "for n in range(N):\n",
    "    indices = list(range(len(features)))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    X = features[indices]\n",
    "    Y = labels[indices]\n",
    "\n",
    "    X_test = X[-test_size:]\n",
    "    Y_test = Y[-test_size:]\n",
    "\n",
    "    X_trains, Y_trains, = [], []\n",
    "\n",
    "    for frac in training_fractions:\n",
    "        training_size = int(frac*len(X))\n",
    "        X_trains.append(X[:training_size])\n",
    "        Y_trains.append(Y[:training_size])\n",
    "\n",
    "    accuracies = []\n",
    "    kernel_inversions = []\n",
    "    pred_times = []\n",
    "    \n",
    "    for i in range(len(training_fractions)):\n",
    "        print(f'Training size {training_fractions[i]*len(X)}')\n",
    "        X_train = X_trains[i]\n",
    "        Y_train = Y_trains[i]\n",
    "\n",
    "        param_grid = {'length': np.logspace(-4,4, num=10),\n",
    "                    'lambda': np.logspace(-12, 0, num=10)}\n",
    "        best_params = GridSearchCV(X_train, Y_train, param_grid, num_features, 4, 1)\n",
    "        print(best_params)\n",
    "\n",
    "        rff = RandomFourierFeatures(input_dim=X_train.shape[1], num_features=num_features, sigma=best_params['length'])\n",
    "\n",
    "        begin = time.time()\n",
    "        #alphas = KRR(X_train, Y_train, best_params['lambda'], best_params['length'])\n",
    "        alphas = KRR_with_RFF(rff.transform(X_train), Y_train, best_params['lambda'], num_features)\n",
    "        end = time.time()\n",
    "        kernel_inversion = end - begin\n",
    "\n",
    "        # normal KRR\n",
    "        #begin2 = time.time()\n",
    "        #dist = scipy.spatial.distance_matrix(X_test,X_train)\n",
    "        #exponent = dist/best_params['length']\n",
    "        #K_test = np.exp(-(exponent**2)/2)\n",
    "        #preds = np.dot(K_test,alphas)>0.5\n",
    "        #end2 = time.time()\n",
    "        #pred_time = end2 - begin2\n",
    "\n",
    "        #accuracies.append(accuracy_score(preds, Y_test))\n",
    "        #kernel_inversions.append(kernel_inversion)\n",
    "        #pred_times.append(pred_time)\n",
    "        # RFF\n",
    "        begin2 = time.time()\n",
    "        rff = RandomFourierFeatures(input_dim=X_train.shape[1], num_features=num_features, sigma=best_params['length'])\n",
    "        X_test_rff = rff.transform(X_test)\n",
    "        preds = X_test_rff @ alphas\n",
    "        end2 = time.time()\n",
    "        pred_time = end2 - begin2\n",
    "        accuracies.append(accuracy_score(preds>0.5, Y_test))\n",
    "        kernel_inversions.append(kernel_inversion)\n",
    "        pred_times.append(pred_time)\n",
    "        print(f'Kernel inversion took {kernel_inversion}, predictions {pred_time}')                                                                                       \n",
    "        \n",
    "    all_accuracies.append(accuracies)\n",
    "    all_kernel_inversions.append(kernel_inversions)\n",
    "    all_pred_times.append(pred_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
